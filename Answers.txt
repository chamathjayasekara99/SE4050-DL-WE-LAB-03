1) The input signal transition 1 to 0.5 , start from 7 to 9  and again move to 1 from 10 .After applying the filter in the result output it shows the changes in the 
input signal.Same approached used for images by computer to identify intensity changes of each pixel of an image. consider values 0 -> 12 are pixel values for image [1,1,1,1,1,1,0.5,0.5,0.5,1,1,1] ,and filer is [1,-1] where filter will be applied across  left to right of image pixel vector.Computer does calculations as below.

(1*1)+(1*(-1)) = 0
-
-
-
6th position
(1*1)+(1*(0.5)) = 0.5
-
-
-
9th position
(0.5*1)+(1*(-1)) =-0.5

Here in 5th to 6th position there is a presence of an edge or transition from darker to lighter pixels , in the other hand from 8th to 9th possion lighter to darker pixels.
After doing these process from entire image computer identify the edges or transition of an image and identify the image eventually.



3) Model get trained too much for the available dataset therefore the problem of Overfitting rise up and validation error increases after some time  at which point we should stop training our dataset. 

we can prevent overfitting by stop trainning too much , weight decay/ regularization with L1/L2 norm , dropout( randomly swich off links randomly according to certain probability , make weight zero),  use different kind of data set, and use more relavant architecture rather than complex one

Since, gradient descent is very computationaly heavy operation, we would not  tipically  apply
the entire dataset in each iteration. Randomly select subset of entire dataset
in each iteration and apply them. This is done to speed up the training without going through entire dataset in each 
iteration to update the weights